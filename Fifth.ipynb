{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install roboflow\n\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"iGSaRG2y43GaizViHQEx\")\nproject = rf.workspace(\"machine-learning-class-eiri5\").project(\"intersection-traffic-piimy\")\nversion = project.version(8)\ndataset = version.download(\"yolov8\")\n                ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Импорты для работы с файлами, обработкой данных и обучения модели\nimport os\nimport shutil\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# PyTorch и torchvision\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom torchvision.models import resnet18\n\n# Для визуализации эмбеддингов\nfrom sklearn.manifold import TSNE\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Пути к датасету\ndataset_path = \"/kaggle/working/Intersection-Traffic--8\"\noutput_path = \"/kaggle/working/intersection_classification\"\n\n# Создаем папки для классификационных данных\ncategories = [\"bicycle\", \"bus\", \"car\", \"motorcycle\"]\nfor category in categories:\n    for split in [\"train\", \"val\", \"test\"]:\n        os.makedirs(os.path.join(output_path, split, category), exist_ok=True)\n\n# Подготовка данных\ndef prepare_classification_data(dataset_path, output_path, test_size=0.2, val_size=0.1):\n    image_paths = list(Path(dataset_path, \"train/images\").rglob(\"*.jpg\"))\n    labels_path = Path(dataset_path, \"train/labels\")\n\n    # Формируем пары (изображение, метка)\n    data = []\n    for image_path in image_paths:\n        label_path = labels_path / (image_path.stem + \".txt\")\n        if label_path.exists():\n            with label_path.open(\"r\") as f:\n                lines = [line.strip().split() for line in f]\n                if lines:\n                    # Берем первую метку в качестве класса\n                    class_idx = int(lines[0][0])\n                    data.append((image_path, categories[class_idx]))\n\n    # Разделяем на train, val, test\n    train_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n    train_data, val_data = train_test_split(train_data, test_size=val_size, random_state=42)\n\n    # Копирование данных\n    def copy_data(split_data, split_name):\n        for image_path, category in split_data:\n            dest_path = Path(output_path, split_name, category, image_path.name)\n            shutil.copy(image_path, dest_path)\n\n    copy_data(train_data, \"train\")\n    copy_data(val_data, \"val\")\n    copy_data(test_data, \"test\")\n\n# Запускаем подготовку данных\nprepare_classification_data(dataset_path, output_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Гиперпараметры\nbatch_size = 32\nlearning_rate = 0.001\nnum_epochs = 30\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Трансформации для данных\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Загрузка данных\ntrain_dataset = datasets.ImageFolder(root=f\"{output_path}/train\", transform=transform)\nval_dataset = datasets.ImageFolder(root=f\"{output_path}/val\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# Модель\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes):\n        super(SimpleCNN, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(32 * 32 * 32, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x\n\n# Инициализация модели\nmodel = SimpleCNN(num_classes=len(categories)).to(device)\n\n# Оптимизатор и функция потерь\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Функция обучения\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # Прямой проход\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Обратный проход\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n        # Валидация\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Accuracy: {100*correct/total:.2f}%\")\n        print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100*val_correct/val_total:.2f}%\")\n\n# Запуск обучения\ntrain_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Функция для извлечения эмбеддингов\ndef extract_embeddings(model, loader, device):\n    model.eval()\n    embeddings = []\n    labels = []\n    with torch.no_grad():\n        for images, targets in loader:\n            images = images.to(device)\n            targets = targets.to(device)\n\n            # Извлекаем эмбеддинги\n            features = model.conv_layers(images)\n            features = features.view(features.size(0), -1)\n            embeddings.append(features.cpu().numpy())\n            labels.append(targets.cpu().numpy())\n    \n    embeddings = np.concatenate(embeddings, axis=0)\n    labels = np.concatenate(labels, axis=0)\n    return embeddings, labels\n\n# Извлечение эмбеддингов\ntrain_embeddings, train_labels = extract_embeddings(model, train_loader, device)\nval_embeddings, val_labels = extract_embeddings(model, val_loader, device)\n\n# Визуализация с TSNE\ndef visualize_embeddings(embeddings, labels, title):\n    tsne = TSNE(n_components=2, random_state=42)\n    reduced_embeddings = tsne.fit_transform(embeddings)\n    plt.figure(figsize=(10, 8))\n    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis', s=5)\n    plt.colorbar(scatter, label=\"Class\")\n    plt.title(title)\n    plt.xlabel(\"TSNE Component 1\")\n    plt.ylabel(\"TSNE Component 2\")\n    plt.show()\n\n# Визуализация тренировочных эмбеддингов\nvisualize_embeddings(train_embeddings, train_labels, \"Train Embeddings (TSNE)\")\n\n# Визуализация валидационных эмбеддингов\nvisualize_embeddings(val_embeddings, val_labels, \"Validation Embeddings (TSNE)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Загружаем предобученную модель\npretrained_model = resnet18(pretrained=True)\n\n# Замораживаем веса backbone\nfor param in pretrained_model.parameters():\n    param.requires_grad = False\n\n# Добавляем кастомный классификатор\nnum_features = pretrained_model.fc.in_features\npretrained_model.fc = nn.Sequential(\n    nn.Linear(num_features, 128),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, len(categories))\n)\npretrained_model = pretrained_model.to(device)\n\n# Оптимизатор и функция потерь\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(pretrained_model.fc.parameters(), lr=learning_rate)\n\n# Обучение модели Transfer Learning\ntrain_model(pretrained_model, train_loader, val_loader, criterion, optimizer, num_epochs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n# Измерение времени обучения для CNN\nstart_time = time.time()\ntrain_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\ncnn_time = time.time() - start_time\n\n# Измерение времени обучения для Transfer Learning\nstart_time = time.time()\ntrain_model(pretrained_model, train_loader, val_loader, criterion, optimizer, num_epochs)\ntransfer_time = time.time() - start_time\n\nprint(f\"Simple CNN training time: {cnn_time:.2f} seconds\")\nprint(f\"Transfer Learning training time: {transfer_time:.2f} seconds\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}